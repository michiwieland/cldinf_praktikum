\input{../template.tex}

% Dokumentinformationen
\newcommand{\SUBJECT}{Report}
\newcommand{\TITLE}{Cloud Infrastructre Lab 4}

\input{../front.tex}

%  03.11.2016, 23:55, as PDF to beat.stettler@ins.hsr.ch. 

% Describe how Qemu KVM and Docker work (technical explanation). Also make sure you highlight the 
% differences, pro’s and con’s. 

% Setup and configuration of the Qemu-KVM environment (with explanations) 

% Qemu network configuration 

% A short manual of how to get the ELK stack up and running in Docker 

% Docker network configuration 

% Docker Compose file for the ELK stack 

% Qemu-KVM and Docker networking configuration files/scripts 
\section{KVM / QEMU}
\subsection{Grundlegendes}
KVM ist eine Virtualisierungslösung für Linux, die Gebrauch von Virtualisierungstechniken aktueller Prozessorarchitekturen macht. Um KVM nutzen zu können, muss der Prozessor des Hostsystems Hardwarevirtualisierung unterstützen (was bei den meisten neueren Intel und AMD Prozessoren der Fall ist). KVM an sich stellt die direkte Schnittstelle zum Linux-Kernel zur Verfügung, als Virtualisierungsumgebung kommt QEMU zum Einsatz.

\subsubsection{Terminologie}
\begin{description}
	\item[Host System] \hfill \\
	Beherbergt eine Virtualisierungslösung und stellt Funktionen zur Verfügung damit ein Gastsystem laufen kann.
	\item[Guest System] \hfill \\
	Läuft virtuell innerhalb des Host Systems. Befehle werden durch eine Virtualisierungslösung an die Hardware des Hosts weitergeleitet.
\end{description}

\subsubsection{QEMU}
QEMU ist ein Open Source System-Emulator. QEMU kommuniziert direkt via KVM mit dem Kernel und der CPU, um so das Gastsystem möglichst nahe an der Hardware laufen zu lassen. Eine komplette Emulation (Vollvirtualisierung) ist mit QEMU ebenfalls möglich, führt jedoch zu grossen Performanceeinbussen.

\subsubsection{Libvirt}

Libvirt ist eine Umgebung, welche den Betrieb einer virtuellen Umgebung über eine einheitliche API zur Verfügung stellt. Dies für KVM/QEMU aber auch alternative Umgebungen wie XEN.

Unter dem Überbegriff \emph{Libvirt} werden normalerweise folgende Komponenten verstanden:

\begin{description}
	\item[libvirt] \hfill \\
	\lstinline|libvirt| ist die eigentliche Programmbibliothek, welche die funktionen der Virtualisierungslösung abstrahiert.
	\item[libvirtd] \hfill \\
	\lstinline|libvirtd| ist ein Daemon, welcher den Betrieb der virtuellen Umgebung (Maschinen, Netzwerken, Speicherpools etc.) organisiert und verwaltet.
	\item[virsh] \hfill \\
	\lstinline|virsh| ist ein Verwaltungswerkzeug, über welches die \lstinline|libvirt(d)|-Umgebung konfiguriert und kontrolliert werden kann. Mit \lstinline|virsh start VM1| kann z.B. eine virtuelle Maschine gestartet werden.
\end{description}

\subsubsection{Virt-Manager}

Das Programm \lstinline|virt-manager| ist ein grafisches Verwaltungsprogramm, über welches die von Libvirt abstrahierte virtuelle Umgebung gesteuert werden kann. Die Oberfläche wird von RedHat in Python/GTK entwickelt und erinnert z.B. an die klassische Oberfläche von VMWare\textregistered vSphere\texttrademark.

Der Virt-Manager kann sich (wie auch \lstinline|virsh|) über eine Netzwerkverbindung mit einer (headless) Libvirt-Umgebung in Verbindung setzen (d.h., es sind keine grafischen Komponenten auf Serverseite nötig.)

\subsection{Installation}
\subsubsection{Hardwarevirtualisierung vmx/svm}
Damit KVM genutzt werden kann, muss die CPU Hardwarevirtualisierung unterstützen. Dies kann bei neueren Prozessoren im UEFI/BIOS aktiviert werden. Ob die Unterstützung momentan aktiv ist, kann mit folgendem Befehl ermittelt werden\footnote{Achtung: Unter gewissen neue Intel CPUs ist dieser Test aufgrund eines Bugs scheinbar erfolgreich, obwohl die Option noch abgestellt ist. In diesem Fall muss die Option trotzdem noch eingeschaltet werden.}:
\begin{lstlisting}[language=bash]
# Eintrag VMX oder SVM sollten zurueckgegeben werden
egrep '(vmx|svm)' /proc/cpuinfo
\end{lstlisting}


Die nötigen \lstinline|kvm|-Kernelmodule werden unter Fedora bei einem Reboot automatisch geladen. Sollte dies nicht der Fall sein, können sie mit folgendem Befehl geladen werden:
\begin{lstlisting}[language=bash]
lsmod | grep kvm # Sollte kvm ausgeben, falls geladen.

# Ansonsten: Kernel Module manuell laden
sudo modprobe kvm
\end{lstlisting}

Bei selber kompilierten Linux Kerneln kann mit \lstinline|make kvmconfig| die entsprechende Build-Konfiguration aktiviert werden.

\subsubsection{Installation KVM}
Die meisten Linux-Distributionen stellen in den Paket-Repositories KVM, QEMU und Libvirt bereit. Wir verwenden in unseren Beispiel die Linux Distribution \lstinline|Fedora Workstation 24|.

\begin{enumerate}
	\item Installation von KVM, QEMU, Libvirt (von Fedora bereitgestelltes Metapaket)\\ \hfill
		\lstinline|sudo dnf install @virtualization libvirt-docs|
	\item Starten des \lstinline|libvirtd|-Dienstes\\ \hfill
		\lstinline|sudo service libvirtd start|
\end{enumerate}

\paragraph{Benutzerzugriff} \hfill \\
Der Benutzer, welcher die Virtuellen Maschinen verwalten soll, muss den Gruppen \lstinline|libvirt| und \lstinline|kvm| hinzugefügt werden (gem. Umgebung von \lstinline|Fedora Workstation 24|)
\begin{lstlisting}[language=bash]
sudo usermod -aG libvirt, kvm $USER
\end{lstlisting}

\subsection{Festplattenimages erstellen}
Festplatten-Images können entweder mit dem Tool \lstinline|qemu-img| erstellt werden, oder alternativ mit Libvirt direkt mit dem Tool \lstinline|virt-install| (siehe nächste Seiten).

In unserem Fall bauen wir aber auf eine bestehendes Festplatten-Image der Mini-Distribution ''CirrOS'' auf.

\subsubsection{Unterstützte Festplattenformate}
QEMU unterstützt als Festplatten Images diverse Formate (die nachfolgende Liste wurde übernommen von \url{https://en.wikibooks.org/wiki/QEMU/Images#Image_types}, Lizenz CC-SA):

\begin{description}
	\item[raw] \hfill \\
	(default) the raw format is a plain binary image of the disc image, and is very portable. On filesystems that support sparse files, images in this format only use the space actually used by the data recorded in them.
	\item[cloop] \hfill \\
	Compressed Loop format, mainly used for reading Knoppix and similar live CD image formats
	\item[cow] \hfill \\
	copy-on-write format, supported for historical reasons only and not available to QEMU on Windows
	\item[qcow] \hfill \\
	the old QEMU copy-on-write format, supported for historical reasons and superseded by qcow2
	\item[qcow2] \hfill \\
	QEMU copy-on-write format with a range of special features, including the ability to take multiple snapshots, smaller images on filesystems that don't support sparse files, optional AES encryption, and optional zlib compression
	\item[vmdk] \hfill \\
	VMware 3 \& 4, or 6 image format, for exchanging images with that product
	\item[vdi] \hfill \\
	VirtualBox 1.1 compatible image format, for exchanging images with VirtualBox.
	\item[vhdx] \hfill \\
	Hyper-V compatible image format, for exchanging images with Hyper-V 2012 or later.
	\item[vpc] \hfill \\
	Hyper-V legacy image format, for exchanging images with Virtual PC / Virtual Server / Hyper-V 2008. 
\end{description}

\subsubsection{Anlegen eines neuen Images ohne \lstinline|virt-install|}
Die Erstellung eines \lstinline|qcow2|-Images Namens \lstinline|test.qcow2| mit einer maximalen Grösse von \lstinline|3GB|. Beim \lstinline|qcow2|-Format wird nicht der ganze Speicherplatz beim erstellen alloziert, sondern erst bei tatsächlicher Belegung\footnote{Damit wird sogenannte ''Überprovisionierung'' möglich, d.h. eine Mehrfachvergabe von tatsächlich verfügbarem Speicherplatz an mehrere VMs. Wichtig: Ist der ''tatsächliche'' Speicherplatz aufgebraucht und das Image hat noch (virtuellen) freien Platz, kann es zu Datenverlust kommen (verursacht durch das Guest-Betriebssystem). Libvirt versetzt zur Vorbeugung in diesem Fall alle VMs in einen standby-Zustand.}.


\begin{lstlisting}[language=bash]
qemu-img create /var/lib/libvirt/images/test.qcow2 3G -f qcow2
virsh pool-refresh default
\end{lstlisting}


\subsubsection{Ablegen eines bestehenden virtuellen Images}

In unserem Beispiel greifen wir auf ein bestehendes Festplatten-Image der Mini-Distribution ''CirrOS'' aus dem OpenStack-Projekt zurück.

Zur Inbetriebnahme laden wir das Image im \lstinline|qcow2|-Format aus dem Internet und speichern es im Ordner \lstinline|/var/lib/libvirt/images/|:

\begin{lstlisting}[language=bash]
sudo wget -O /var/lib/libvirt/images/vm1.qcow2 \
http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
virsh pool-refresh default
\end{lstlisting}

\subsubsection{Libvirt Storage Pools}

Wir haben beim oben stehenden Hinzufügen von Images jeweils zusätzlich das Kommando \lstinline|virsh pool-refresh default| ausgeführt. Dadurch wird der Libvirt Storage-Pool aktualisiert, damit das neue Image erkannt wird.

Libvirt verwaltet die Speicherorte, von denen virtuelle Images Eingebungen werden können, in sogenannten \emph{Pools}. Ein Pool ist dabei oft ein bestimmter Ordner auf dem Computer oder einem Netzwerklaufwerk, kann aber z.B. auch ein iSCSI Speicher darstellen.

Standardmässig ist unter Fedora der Ordner \lstinline|/var/lib/libvirt/images/| mit dem Namen \lstinline|default| als Pool eingerichtet. Diesen haben wir in unseren Beispielen immer verwendet.

Die Pools können mit \lstinline|virsh| wie folgt verwendet werden (Ausschnitt aus \lstinline|virsh help|):

\lstinputlisting{appendix/kvm/libvirt/pool_virsh.man}

Die XML-Konfiguration des \lstinline|default|-Pools sieht wie folgt aus:

\lstinputlisting[language=xml]{appendix/kvm/libvirt/pool_default.xml}



\subsection{Erstellen der virtuellen Maschinen}


\subsubsection{Virt-Install}
Um eine neue virtuelle Maschine anzulegen, stellt Libvirt das Tool \lstinline|virt-install| zur Verfügung.

In unserem Beispiel setzten wir eine einfache virtuelle Maschine auf namens \lstinline|VM1|, \lstinline|512|MB RAM und dem gleichen emulierten CPU-Typ wie das Hostsystem.

Den Parameter \lstinline|--os-variant ubuntu14.04| brauchen wir hier, um die Standardeinstellungen auf ein ähnliches Linux-System einzustellen.

Mittels der Grafik- und Serial-Konfiguration deaktivieren wir die Erstellung einer virtuellen Grafikkarte und setzen stattdessen eine Serielle Schnittstelle ein; diese genügt für unsere Zwecke.

Mit \lstinline|--import --disk| geben wir das bestehende Festplattenimage an, da wir kein neues Image erstellen möchten. Alternativ wäre mit \lstinline|--disk size=3| die Erstellung eines leeren 3GB-Image im \lstinline|default|-Pool mit dem Namen der virtuellen Maschine möglich.

\begin{lstlisting}[language=bash]
virt-install --name VM1 --ram 512 --cpu host\
--os-variant ubuntu14.04\
--graphics none --noautoconsole --serial pty \
--import --disk /var/lib/libvirt/images/vm1.qcow2
\end{lstlisting}

\lstinline|virt-install| legt mit diesem Kommando eine Libvirt-XML-Konfiguration für die virtuelle Maschine an (siehe Abschnitt \ref{sec:libvirt-xml-konfiguration-eines-gastes}).

Nach dem Anlegen startet \lstinline|virt-install| die VM direkt.

%TODO: Wie kann das verhindert werden bzw. andernfalls die VM wieder heruntergefahren?

\subsubsection{Installation ohne \lstinline|virt-install|}

Alternativ zur Generierung der Konfiguration mit \lstinline|virt-install| gibt es die Möglichkeit, eine XML-Konfiguration  direkt mit \lstinline|virsh| zu importieren:

\begin{lstlisting}
virsh create /root/VM1.xml
\end{lstlisting}

\lstinline|virsh| legt damit, gleich wie \lstinline|virt-install|, die Konfigurationsdatei der VM unter \lstinline|/etc/libvirt/qemu/VM1.xml| ab. Vorgängig führt Libvirt eine Validierung der Konfiguration durch.

(Siehe Abschnitt \ref{sec:libvirt-xml-konfiguration-eines-gastes} für die XML-Konfiguration)

\subsubsection{Libvirt XML-Konfiguration eines Gastes}\label{sec:libvirt-xml-konfiguration-eines-gastes}

Die Datei \lstinline|/etc/libvirt/qemu/VM1.xml| (bzw. \lstinline|/root/VM1.xml|) sieht folgendermassen aus:
\lstinputlisting{appendix/kvm/libvirt/domain_VM1.xml}

\subsubsection{Virt-Clone}
%TODO: Clone VMs with \lstinline|virt-clone| and without.


\subsection{Netzwerk Konfiguration}
% TODO Remove sources
% http://wiki.qemu.org/Documentation/Networking#Network_backend_types
% http://wiki.libvirt.org/page/Networking#Debian.2FUbuntu_Bridging


\subsubsection{Grundlegend}
%TODO 
Für jedes virtuelle Netzwerk legt libvirt eine virtuelle Bridge an. Diese können auf dem Host mit \lstinline|brctl show| angezeigt werden. Zudem wird ein virtuelles Interface im zugewiesenen Adressraum (default: 192.168.122.0/24) erstellt, das per NAT mit der physischen Netzwerkkarte verbunden ist. Damit jeder virtuelle Maschine eine gültige IP Adresse kriegt, wird auf dem Host Interface ein DHCP Server (dnsmasq) eingerichtet. Mit dieser Konfiguration kann jede virtuelle Maschine ins Internet, hat jedoch keine Konnektivität zu den anderen VM's. 

\begin{description}
	\item[/var/lib/libvirt/dnsmasq] Die DHCP Konfigurations-Files 
	\item[/etc/libvirt/qemu/networks] Netzwerk Konfigurations-Files
\end{description}

\subsubsection{Host einrichten}
\begin{lstlisting}[language=bash]
virsh net-define /usr/share/libvirt/networks/default.xml
virsh net-autostart default
virsh net-start default
virsh net-edit <network-name>
\end{lstlisting}


\lstinline|/etc/libvirt/qemu/networks|

\subsubsection{Bridge Erstellen}
\begin{lstlisting}[language=xml]
<network>
	<name>VM1_VM2</name>
	<uuid>b7aca73a-21b9-43af-8c91-0af4f95bd29b</uuid>
	<bridge name='virbr1' stp='on' delay='0'/>
	<mac address='52:54:00:1e:b6:ae'/>
	<domain name='VM1_VM2'/>
	<ip address='192.168.100.1' netmask='255.255.255.0'>
	<dhcp>
	<range start='192.168.100.128' end='192.168.100.254'/>
	</dhcp>
	</ip>
</network>
\end{lstlisting}

\subsection{Verwalten einer VM mit \lstinline|virsh|}

%TODO: Start, Console, Shutdown (ACPI/Forced), further management

\begin{lstlisting}[language=bash]
virsh create /etc/libvirt/qemu/VM1.xml

virsh start VM1 

virsh console VM1
\end{lstlisting}

\subsection{KVM / QEMU unter der Libvirt-Abstraktion}
%TODO: Ansatzweise Beschreiben, wie KVM/QEMU auch ohne Libvirt verwendet werden kann



\section{Docker}
\subsection{Grundlegendes}
Docker ist im Gegensatz zu KVM keine Lösung für virtuelle Maschinen (und damit unabhängige Betriebssysteme mit eigenem CPU-Scheduling etc.) sondern basiert auf Linux Container (LXC). Ein LXC ist eine vom Kernel bereitgestellte virtuelle Umgebung zur isolierten Ausführung von Prozessen.

Docker ermöglicht neben der Steuerung von isolierten Applikationsausführung in LXC die Konfiguration von Netzwerkeinstellungen bzw. Portfreigabe ein komplett isoliertes Dateisystem pro Applikation (sogenannte Images). Damit ist es möglich, Systemunabhängig immer die gleiche Plattform zur Ausführung zu paketieren und weiter zu verteilen. Von Docker erstellte Images werden üblicherweise mit einem Dateisystem erstellt, welches Snapshots erlaubt (z.B. AUFS, UnionFS oder BTRFS).

Zur Erstellung von eigenen Images, auf deren Basis eine Applikation ausgeführt wird, können mit einem \lstinline[]|Dockerfile| komplette ''Bauanleitungen'' erstellt werden. Im \lstinline[]|Dockerfile| sind die Eigenschaften des neuen Container und Images sowie darauf auszuführende Kommandos zur Bereitstellung der Umgebung hinterlegt.

So gebaute Images (oft in diesem Zusammenhang auch einfach als Container bezeichnet) können über eine Online-Plattform verteilt werden. Die Firma Docker Inc. stellt zu diesem Zweck die öffentliche Plattform \emph{DockerHub} unter \url{https://hub.docker.com/} zur Verfügung.

Beim Bauen eines Containers wird üblicherweise auf die Basis eines bestehenden Containers auf der DockerHub-Plattform zurückgegriffen. So gibt es dort zum Beispiel Images, welche die Dateisystemstruktur und Bibliotheken eines minimalen Ubuntu-Systems oder sogar einer Umgebung für  wie Ruby on Rails zur Verfügung stellen.


\subsubsection{Terminologie}
\begin{description}
	\item[Container, VDI: Virtual Disk Image] \hfill \\
	Virtuelle Festplatte des Gastsystems. Es gibt statische und dynamische Container.
	%TODO: Unter Container wird z.T. auch die Konfiguration einbezogen.
\end{description}


\subsubsection{Docker Compose}

Docker Compose ist ein ergänzendes Programm zu Docker, welches die Provisionierung und Verknüpfung von mehreren Docker-Containern erlaubt. Der Aufbau/Start der Container wird mittels dem Kommando \lstinline|docker-compose| gesteuert; dies liest die \lstinline|docker-compose.yml|-Datei ein, in welcher weitergehende die Anweisungen für die Konfiguration der Container definiert sind.
%TODO Michi: Fehlt hier noch etwas?

\subsubsection{Docker Swarm}
Docker Swarm ist ein Orchestrierungs-Werkzeug für den Betrieb von Docker-Clustern (oder auch einzelnen Docker Hosts).
%TODO Michi: Was kann das noch mehr?.



\subsection{Docker IO}
\begin{lstlisting}[language=bash]
# docker container suchen
docker sarch <package-name>

# docker container herunterladen
docker pull <package-name>

# Image veroeffentlichen
docker push <image name>

# docker container starten
docker run <package-name> <app to execute>

# Aktuell ausgefuerten Container anzeigen
docker ps

# Liste alle lokal vorliegenden Images
docker images

# Image entfernen
docker rmi <image>

# Container entfernen
docker rm <container>
\end{lstlisting}


\subsection{Beschreibung}
\paragraph{Vorteile}
\begin{itemize}
	\item Äusserts sparsamer Umgang mit Ressourcen und eine kurze Startzeit	
	\item Mit Hilfe von Vagrant äusserts einfach einen Docker Container zu installieren
	\item Austausch von Images fällt durch den Gebrauch von öffentlichen Registries leicht.
\end{itemize}
\paragraph{Nachteile}
\begin{itemize}
	\item Da die Container auf den Linux Kernel zugreifen, ist es nicht möglich Windows Anwendungen in einem Docker Container auszuführen.
	\item Einschränkung auf 64Bit basiertes Linux oder Mac OSX (%TODO immer noch=? -> Meines Wissens schon!
\end{itemize}

\subsection{ELK Stack Setup (Elasticseach, Logstash, Kibana)}
\subsubsection{Vorraussetzungen}
\paragraph{SELinux} \hfill \\
Da wir mit einem Fedora arbeiten, müssen vorgän
\begin{lstlisting}[language=bash]
chcon -R system_u:object_r:admin_home_t:s0 docker-elk
\end{lstlisting}



\subsection{Network Configuration}

\subsection{Docker Compose File ELK Stack}




\section{Vergleich der Technologien}
%TODO

\subsection{KVM} %TODO
\paragraph{Vorteile}
\begin{itemize}
	\item Ein Vorteil von KVM ist, dass die Gastsysteme fast mit nativer Geschwindigkeit laufen, d.h. das Gastsystem reagiert nahezu so schnell wie ein natives System. 
\end{itemize}
\paragraph{Nachteile}
\begin{itemize}
	\item Hoher Bedarf an Festplatten und Arbeitsspeicher und langwieriger Ladevorgang beim Starten der virtuellen Maschienen
\end{itemize}

\input{appendix.tex}
